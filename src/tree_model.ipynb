{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb/ lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "arrest_data = pd.read_csv('../data/chicago_clean_final_arrest.csv')\n",
    "non_arrest_data = pd.read_csv('../data/chicago_clean_final_not_arrest.csv')\n",
    "\n",
    "# Convert date column to datetime\n",
    "arrest_data['Date'] = pd.to_datetime(arrest_data['Date'], format='%Y-%m-%d %H:%M:%S')\n",
    "non_arrest_data['Date'] = pd.to_datetime(non_arrest_data['Date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Set date as index if it's not already\n",
    "if 'Date' in arrest_data.columns:\n",
    "    arrest_data = arrest_data.set_index('Date')\n",
    "    non_arrest_data = non_arrest_data.set_index('Date')\n",
    "\n",
    "# Define our prediction goal\n",
    "# We will predict whether a specific grid will have more than the median crime rate in the next 24 hours\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "# 1. Temporal Features\n",
    "def add_temporal_features(df):\n",
    "    \"\"\"Add time-based features to the dataframe.\"\"\"\n",
    "    df_temp = df.reset_index()\n",
    "    \n",
    "    # Basic time features\n",
    "    df_temp['hour'] = df_temp['Date'].dt.hour\n",
    "    df_temp['day_of_week'] = df_temp['Date'].dt.dayofweek\n",
    "    df_temp['day_of_month'] = df_temp['Date'].dt.day\n",
    "    df_temp['month'] = df_temp['Date'].dt.month\n",
    "    df_temp['year'] = df_temp['Date'].dt.year\n",
    "    df_temp['is_weekend'] = df_temp['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Time of day categories\n",
    "    df_temp['time_of_day'] = pd.cut(\n",
    "        df_temp['hour'], \n",
    "        bins=[0, 6, 12, 18, 24], \n",
    "        labels=['night', 'morning', 'afternoon', 'evening']\n",
    "    )\n",
    "    \n",
    "    # One-hot encode time of day\n",
    "    time_of_day_dummies = pd.get_dummies(df_temp['time_of_day'], prefix='time')\n",
    "    df_temp = pd.concat([df_temp, time_of_day_dummies], axis=1)\n",
    "    \n",
    "    # Holidays (US holidays - simplified for demo)\n",
    "    holidays = [\n",
    "        # New Year's Day\n",
    "        pd.Timestamp(year=y, month=1, day=1) for y in range(2001, 2026)\n",
    "    ] + [\n",
    "        # Independence Day\n",
    "        pd.Timestamp(year=y, month=7, day=4) for y in range(2001, 2026)\n",
    "    ] + [\n",
    "        # Christmas\n",
    "        pd.Timestamp(year=y, month=12, day=25) for y in range(2001, 2026)\n",
    "    ]\n",
    "    \n",
    "    df_temp['is_holiday'] = df_temp['Date'].isin(holidays).astype(int)\n",
    "    \n",
    "    return df_temp.set_index('Date')\n",
    "\n",
    "# 2. Spatial Features - Add grid coordinates \n",
    "def add_spatial_features(df):\n",
    "    \"\"\"Extract spatial features from the grid columns.\"\"\"\n",
    "    grid_columns = df.columns\n",
    "    \n",
    "    # Create features for each grid\n",
    "    for grid in grid_columns:\n",
    "        if '_' not in grid:\n",
    "            continue\n",
    "        \n",
    "        x, y = map(int, grid.split('_'))\n",
    "        \n",
    "        # Create grid distance feature (distance from city center, assuming grid 5_5 is downtown)\n",
    "        center_x, center_y = 5, 5\n",
    "        df[f'dist_from_center'] = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "        \n",
    "        # Create grid location features\n",
    "        df[f'is_north'] = int(y > 5)\n",
    "        df[f'is_south'] = int(y < 5)\n",
    "        df[f'is_east'] = int(x > 5)\n",
    "        df[f'is_west'] = int(x < 5)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 3. Rolling Window Features\n",
    "def add_rolling_features(df, windows=[1, 3, 6, 12, 24, 48, 168]):\n",
    "    \"\"\"Add rolling window statistics as features.\"\"\"\n",
    "    df_rolling = df.copy()\n",
    "    \n",
    "    for grid in df.columns:\n",
    "        if '_' not in grid:\n",
    "            continue\n",
    "            \n",
    "        for window in windows:\n",
    "            # Rolling means\n",
    "            df_rolling[f'roll_mean_{window}h'] = df[grid].rolling(window=window, min_periods=1).mean()\n",
    "            \n",
    "            # Rolling standard deviations (volatility)\n",
    "            df_rolling[f'roll_std_{window}h'] = df[grid].rolling(window=window, min_periods=1).std().fillna(0)\n",
    "            \n",
    "            # Rolling maximum\n",
    "            df_rolling[f'roll_max_{window}h'] = df[grid].rolling(window=window, min_periods=1).max()\n",
    "    \n",
    "    return df_rolling\n",
    "\n",
    "# 4. Lagged Features\n",
    "def add_lag_features(df, lags=[1, 3, 6, 12, 24, 48, 168]):\n",
    "    \"\"\"Add lagged values as features.\"\"\"\n",
    "    df_lag = df.copy()\n",
    "    \n",
    "    for grid in df.columns:\n",
    "        if '_' not in grid:\n",
    "            continue\n",
    "            \n",
    "        for lag in lags:\n",
    "            df_lag[f'lag_{lag}h'] = df[grid].shift(lag)\n",
    "    \n",
    "    # Fill NaN values with 0 (for the beginning of the time series)\n",
    "    df_lag = df_lag.fillna(0)\n",
    "    return df_lag\n",
    "\n",
    "# 5. Interaction Features\n",
    "def add_interaction_features(df, arrest_df):\n",
    "    \"\"\"Add interaction features between grids and arrest rate.\"\"\"\n",
    "    # Calculate arrest rate (arrest / (arrest + non-arrest))\n",
    "    all_crime = arrest_df + df\n",
    "    arrest_rate = arrest_df / all_crime.replace(0, 1)  # Prevent division by zero\n",
    "    \n",
    "    df_interact = df.copy()\n",
    "    \n",
    "    for grid in df.columns:\n",
    "        if '_' not in grid:\n",
    "            continue\n",
    "            \n",
    "        # Add arrest rate feature\n",
    "        df_interact[f'arrest_rate'] = arrest_rate[grid]\n",
    "        \n",
    "        # Add neighbor interaction\n",
    "        x, y = map(int, grid.split('_'))\n",
    "        \n",
    "        # Find neighboring grids\n",
    "        neighbors = []\n",
    "        for dx, dy in [(0,1), (1,0), (0,-1), (-1,0)]:  # Up, right, down, left\n",
    "            nx, ny = x + dx, y + dy\n",
    "            neighbor = f\"{nx}_{ny}\"\n",
    "            if neighbor in df.columns:\n",
    "                neighbors.append(neighbor)\n",
    "        \n",
    "        # If grid has neighbors, create features\n",
    "        if neighbors:\n",
    "            # Crime spread from neighbors\n",
    "            df_interact[f'neighbor_sum'] = df[[n for n in neighbors]].sum(axis=1)\n",
    "            \n",
    "            # Arrest rate of neighbors\n",
    "            neighbor_arrest_rate = arrest_rate[[n for n in neighbors]].mean(axis=1)\n",
    "            df_interact[f'neighbor_arrest_rate'] = neighbor_arrest_rate\n",
    "    \n",
    "    return df_interact\n",
    "\n",
    "# Function to prepare target variable\n",
    "def prepare_target(df, threshold_percentile=50, prediction_window=24):\n",
    "    \"\"\"\n",
    "    Create target variable: 1 if crime is above threshold in next window, else 0\n",
    "    \"\"\"\n",
    "    # Calculate future crime counts (sum over next prediction_window hours)\n",
    "    future_crime = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for grid in df.columns:\n",
    "        if '_' not in grid:\n",
    "            continue\n",
    "            \n",
    "        # Sum crime counts over next prediction_window hours\n",
    "        future_crime[grid] = df[grid].rolling(window=prediction_window).sum().shift(-prediction_window)\n",
    "    \n",
    "    # Determine threshold for each grid\n",
    "    thresholds = {}\n",
    "    for grid in future_crime.columns:\n",
    "        if '_' not in grid:\n",
    "            continue\n",
    "        thresholds[grid] = np.percentile(future_crime[grid].dropna(), threshold_percentile)\n",
    "    \n",
    "    # Create binary target: 1 if crime is above threshold, 0 otherwise\n",
    "    target = pd.DataFrame(index=future_crime.index)\n",
    "    for grid in future_crime.columns:\n",
    "        if '_' not in grid:\n",
    "            continue\n",
    "        target[f'target'] = (future_crime[grid] > thresholds[grid]).astype(int)\n",
    "    \n",
    "    return target\n",
    "\n",
    "# Create a model training and evaluation pipeline\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, model_type='xgboost'):\n",
    "    \"\"\"Train and evaluate model for crime prediction.\"\"\"\n",
    "    \n",
    "    if model_type == 'xgboost':\n",
    "        model = xgb.XGBClassifier(\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif model_type == 'lightgbm':\n",
    "        model = lgb.LGBMClassifier(\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "    # elif model_type == 'randomforest':\n",
    "    #     model = RandomForestClassifier(\n",
    "    #         n_estimators=100,\n",
    "    #         max_depth=5,\n",
    "    #         min_samples_split=10,\n",
    "    #         min_samples_leaf=5,\n",
    "    #         random_state=42\n",
    "    #     )\n",
    "    # elif model_type == 'logistic':\n",
    "    #     # Scale features for logistic regression\n",
    "    #     scaler = StandardScaler()\n",
    "    #     X_train = scaler.fit_transform(X_train)\n",
    "    #     X_test = scaler.transform(X_test)\n",
    "        \n",
    "    #     model = LogisticRegression(\n",
    "    #         C=0.1,\n",
    "    #         penalty='l2',\n",
    "    #         solver='lbfgs',\n",
    "    #         max_iter=1000,\n",
    "    #         random_state=42\n",
    "    #     )\n",
    "    else:\n",
    "        raise ValueError(\"Model type not supported\")\n",
    "    \n",
    "    # Add this before the model.fit() in your train_evaluate_model function\n",
    "    print(\"X_train info:\")\n",
    "    print(X_train.info())\n",
    "    print(\"X_train columns with object dtype:\")\n",
    "    print(X_train.select_dtypes(include=['object']).columns.tolist())\n",
    "    print(\"X_train sample:\")\n",
    "    print(X_train.head())\n",
    "\n",
    "    # If object columns exist, convert them\n",
    "    for col in X_train.select_dtypes(include=['object']).columns:\n",
    "        X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "        X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "    X_train_np = X_train.values.astype(np.float32)\n",
    "    X_test_np = X_test.values.astype(np.float32)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_np, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred_proba = model.predict_proba(X_test_np)[:, 1]\n",
    "    y_pred = model.predict(X_test_np)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"Model: {model_type}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(\"Top 10 important features:\")\n",
    "        print(feature_importance.head(10))\n",
    "    \n",
    "    return model, y_pred_proba\n",
    "\n",
    "# This will be our main pipeline function that puts everything together\n",
    "def crime_prediction_pipeline(grid_to_predict='5_5'):\n",
    "    \"\"\"\n",
    "    Run the complete crime prediction pipeline for a specific grid.\n",
    "    \"\"\"\n",
    "    print(f\"Preparing prediction model for grid {grid_to_predict}...\")\n",
    "    \n",
    "    # 1. Feature Engineering\n",
    "    print(\"Adding temporal features...\")\n",
    "    arrest_temp = add_temporal_features(arrest_data)\n",
    "    non_arrest_temp = add_temporal_features(non_arrest_data)\n",
    "    \n",
    "    # Keep only the grid column we want to predict\n",
    "    arrest_grid = arrest_temp[[grid_to_predict]]\n",
    "    non_arrest_grid = non_arrest_temp[[grid_to_predict]]\n",
    "    \n",
    "    print(\"Adding rolling and lag features...\")\n",
    "    arrest_features = add_rolling_features(arrest_grid)\n",
    "    arrest_features = add_lag_features(arrest_features)\n",
    "    \n",
    "    non_arrest_features = add_rolling_features(non_arrest_grid)\n",
    "    non_arrest_features = add_lag_features(non_arrest_features)\n",
    "    \n",
    "    # Create target variable\n",
    "    print(\"Preparing target variable...\")\n",
    "    target = prepare_target(non_arrest_grid)\n",
    "    \n",
    "    # Combine features and ensure same index\n",
    "    all_features = pd.concat([non_arrest_features, arrest_features], axis=1)\n",
    "    all_features = all_features.join(arrest_temp[['hour', 'day_of_week', 'month', 'is_weekend', \n",
    "                                                'time_night', 'time_morning', 'time_afternoon', 'time_evening',\n",
    "                                                'is_holiday']])\n",
    "    \n",
    "    # Get target for the specific grid\n",
    "    y = target[f'{grid_to_predict}_target']\n",
    "    \n",
    "    # Align features and target\n",
    "    all_features = all_features.loc[y.index]\n",
    "    y = y.loc[all_features.index]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    mask = ~y.isna()\n",
    "    all_features = all_features[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    # Feature selection - remove features with more than 50% missing values\n",
    "    missing_ratio = all_features.isna().mean()\n",
    "    all_features = all_features.drop(columns=missing_ratio[missing_ratio > 0.5].index)\n",
    "    \n",
    "    # Fill remaining missing values\n",
    "    all_features = all_features.fillna(0)\n",
    "    \n",
    "    # 2. Train-Test Split (Time-based)\n",
    "    print(\"Splitting data for training and testing...\")\n",
    "    train_size = int(len(all_features) * 0.8)\n",
    "    X_train = all_features.iloc[:train_size]\n",
    "    X_test = all_features.iloc[train_size:]\n",
    "    y_train = y.iloc[:train_size]\n",
    "    y_test = y.iloc[train_size:]\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "    # print(X_train.dtypes)\n",
    "    \n",
    "    # 3. Model Training and Evaluation\n",
    "    print(\"Training models...\")\n",
    "    models = {}\n",
    "    # for model_type in ['xgboost', 'lightgbm', 'randomforest', 'logistic']:\n",
    "    for model_type in ['lightgbm']:\n",
    "        print(f\"\\nTraining {model_type} model...\")\n",
    "        model, y_pred_proba = train_evaluate_model(X_train, X_test, y_train, y_test, model_type)\n",
    "        models[model_type] = model\n",
    "    \n",
    "    return models, all_features, y\n",
    "\n",
    "\n",
    "# Execute the pipeline for a specific grid (e.g., downtown area)\n",
    "# models, features, target = crime_prediction_pipeline('0_7')\n",
    "\n",
    "# Plot the predictions over time for the best model\n",
    "# best_model = models['lightgbm']\n",
    "# y_pred_proba = best_model.predict_proba(features.iloc[int(len(features) * 0.8):])[:, 1]\n",
    "\n",
    "# Create a time series plot with predictions\n",
    "# plt.figure(figsize=(15, 8))\n",
    "# plt.plot(target.iloc[int(len(target) * 0.8):].index, target.iloc[int(len(target) * 0.8):], label='Actual')\n",
    "# plt.plot(target.iloc[int(len(target) * 0.8):].index, y_pred_proba, label='Predicted Probability')\n",
    "# plt.title('Crime Prediction in Grid 5_5 (Downtown Chicago)')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Crime Occurrence Probability')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified function to prepare data for all grids\n",
    "def prepare_all_grids_data():\n",
    "    \"\"\"\n",
    "    Prepare data for all grids at once to train a single model\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    from pandas.errors import PerformanceWarning\n",
    "    \n",
    "    # Suppress PerformanceWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=PerformanceWarning)\n",
    "    \n",
    "    print(\"Preparing prediction model for all grids...\")\n",
    "    \n",
    "    # 1. Add temporal features\n",
    "    print(\"Adding temporal features...\")\n",
    "    arrest_temp = add_temporal_features(arrest_data)\n",
    "    non_arrest_temp = add_temporal_features(non_arrest_data)\n",
    "    \n",
    "    # Get all grid columns\n",
    "    grid_columns = [col for col in non_arrest_data.columns if '_' in col][:2]\n",
    "    \n",
    "    print(f\"Processing {len(grid_columns)} grids...\")\n",
    "    \n",
    "    # 2. Create a combined dataframe for all grids\n",
    "    all_data = []\n",
    "    \n",
    "    for grid in grid_columns:\n",
    "        print(f\"Processing grid {grid}...\")\n",
    "        \n",
    "        # Extract data for this grid\n",
    "        arrest_grid = arrest_temp[[grid]].rename(columns={grid: 'grid_value'})  # Rename to standard column name\n",
    "        non_arrest_grid = non_arrest_temp[[grid]].rename(columns={grid: 'grid_value'})  # Rename to standard column name\n",
    "        \n",
    "        # Add rolling and lag features\n",
    "        arrest_features = add_rolling_features(arrest_grid)\n",
    "        arrest_features = add_lag_features(arrest_features)\n",
    "        \n",
    "        non_arrest_features = add_rolling_features(non_arrest_grid)\n",
    "        non_arrest_features = add_lag_features(non_arrest_features)\n",
    "        \n",
    "        # Create target variable\n",
    "        target = prepare_target(non_arrest_grid)\n",
    "        \n",
    "        # Combine features\n",
    "        features = pd.concat([non_arrest_features, arrest_features], axis=1)\n",
    "        features = features.join(arrest_temp[['hour', 'day_of_week', 'month', 'is_weekend', \n",
    "                                           'time_night', 'time_morning', 'time_afternoon', 'time_evening',\n",
    "                                           'is_holiday']])\n",
    "        \n",
    "        # Get target for this grid\n",
    "        y = target[f'target']\n",
    "        \n",
    "        # Align features and target\n",
    "        features = features.loc[y.index]\n",
    "        y = y.loc[features.index]\n",
    "        \n",
    "        # Remove NaN values\n",
    "        mask = ~y.isna()\n",
    "        features = features[mask]\n",
    "        y = y[mask]\n",
    "        \n",
    "        # Add grid identifier\n",
    "        features['grid_id'] = grid\n",
    "        features['target'] = y.values\n",
    "        \n",
    "        # Reset index to avoid duplicate indices when concatenating\n",
    "        features = features.reset_index()\n",
    "        \n",
    "        # Append to our collection\n",
    "        all_data.append(features)\n",
    "    \n",
    "    print(len(all_data))\n",
    "    for df in all_data:\n",
    "        print(df.columns.tolist())\n",
    "        print(df.head(3))\n",
    "        print(df.index.is_unique)\n",
    "    \n",
    "\n",
    "    # Combine all grid data into one dataframe\n",
    "    combined_data = pd.concat(all_data, axis=0, ignore_index=True)\n",
    "    \n",
    "    # Feature selection - remove features with more than 50% missing values\n",
    "    missing_ratio = combined_data.isna().mean()\n",
    "    combined_data = combined_data.drop(columns=missing_ratio[missing_ratio > 0.5].index)\n",
    "    \n",
    "    # Fill remaining missing values\n",
    "    combined_data = combined_data.fillna(0)\n",
    "    \n",
    "    # Extract target\n",
    "    y = combined_data['target']\n",
    "    X = combined_data.drop(columns=['target'])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Modified function to train model on all grids\n",
    "def train_model_all_grids():\n",
    "    \"\"\"Train a single model on data from all grids.\"\"\"\n",
    "    import warnings\n",
    "    from pandas.errors import PerformanceWarning\n",
    "    \n",
    "    # Suppress PerformanceWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=PerformanceWarning)\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y = prepare_all_grids_data()\n",
    "    \n",
    "    # Time-based split\n",
    "    print(\"Splitting data for training and testing...\")\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train = X.iloc[:train_size]\n",
    "    X_test = X.iloc[train_size:]\n",
    "    y_train = y.iloc[:train_size]\n",
    "    y_test = y.iloc[train_size:]\n",
    "    \n",
    "    print(f\"Training data shapes: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"Testing data shapes: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    \n",
    "    # Handle different data types\n",
    "    \n",
    "    # 1. First identify timestamp/datetime columns and convert them properly\n",
    "    for col in X_train.select_dtypes(include=['datetime64']).columns:\n",
    "        # Extract useful features from timestamps instead of using raw timestamps\n",
    "        X_train[f'{col}_hour'] = X_train[col].dt.hour\n",
    "        X_train[f'{col}_day'] = X_train[col].dt.day\n",
    "        X_train[f'{col}_month'] = X_train[col].dt.month\n",
    "        X_train[f'{col}_year'] = X_train[col].dt.year\n",
    "        \n",
    "        X_test[f'{col}_hour'] = X_test[col].dt.hour\n",
    "        X_test[f'{col}_day'] = X_test[col].dt.day\n",
    "        X_test[f'{col}_month'] = X_test[col].dt.month\n",
    "        X_test[f'{col}_year'] = X_test[col].dt.year\n",
    "        \n",
    "        # Drop the original timestamp column\n",
    "        X_train = X_train.drop(columns=[col])\n",
    "        X_test = X_test.drop(columns=[col])\n",
    "    \n",
    "    # 2. Convert any object columns to numeric\n",
    "    for col in X_train.select_dtypes(include=['object']).columns:\n",
    "        X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "        X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "    \n",
    "    # 3. Handle index column if it exists\n",
    "    if 'index' in X_train.columns:\n",
    "        X_train = X_train.drop(columns=['index'])\n",
    "        X_test = X_test.drop(columns=['index'])\n",
    "    \n",
    "    # 4. Convert grid_id to numeric if it's categorical\n",
    "    if 'grid_id' in X_train.columns and X_train['grid_id'].dtype == 'object':\n",
    "        # Create a label encoder for the grid_id\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        X_train['grid_id'] = le.fit_transform(X_train['grid_id'])\n",
    "        X_test['grid_id'] = le.transform(X_test['grid_id'])\n",
    "    \n",
    "    # Fill NA values\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "    \n",
    "    # Final check for numeric types\n",
    "    for col in X_train.columns:\n",
    "        if not np.issubdtype(X_train[col].dtype, np.number):\n",
    "            print(f\"Warning: Column {col} is not numeric, converting...\")\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_np = X_train.values.astype(np.float32)\n",
    "    X_test_np = X_test.values.astype(np.float32)\n",
    "    \n",
    "    # Train LightGBM model\n",
    "    print(\"Training LightGBM model on all grids...\")\n",
    "    model = lgb.LGBMClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=200,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_np, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred_proba = model.predict_proba(X_test_np)[:, 1]\n",
    "    y_pred = model.predict(X_test_np)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    return model, X_train, y_train\n",
    "\n",
    "# Run the all-grids pipeline\n",
    "model, X, y = train_model_all_grids()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it5006",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
